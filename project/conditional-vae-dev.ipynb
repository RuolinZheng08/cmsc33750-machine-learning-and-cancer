{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "# %load utils.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print('Cuda:', torch.cuda.is_available())\n",
    "torch.manual_seed(0)\n",
    "#torch.set_deterministic(True)\n",
    "np.random.seed(0)\n",
    "\n",
    "INDIR = '../../input/'\n",
    "OUTDIR = '../../output'\n",
    "N_IN_CHANNELS = 3 # RGB\n",
    "N_CLASSES = 2 # binary classification\n",
    "N_LATENT = 100\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 96\n",
    "CROP_SIZE = 64\n",
    "N_ROW_IMG = 4 # show 4x4 grid of generated img\n",
    "\n",
    "def imshow(x):\n",
    "    img = x.data.cpu().permute(1, 2, 0).numpy()\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.annotations.id[index] + '.tif'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        y_label = torch.tensor(self.annotations.label[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n",
    "\n",
    "class ConditionalConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, n_in_channels, n_classes):\n",
    "        super(ConditionalConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        n_channels = 16 # tuneable hyperparam\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_in_channels + n_classes, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.flat_dim = n_channels * 8 * 4 * 4\n",
    "\n",
    "        self.mu = nn.Linear(self.flat_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(self.flat_dim, latent_dim)\n",
    "\n",
    "        self.decoder_fc = nn.Linear(latent_dim + n_classes, self.flat_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 2, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels, n_in_channels, 4, 2, 1),\n",
    "        )\n",
    "\n",
    "    def encode(self, input):\n",
    "        # implementation goes here\n",
    "        x = self.encoder(input)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        # implementation goes here\n",
    "        epsilon = torch.normal(0., 1., size=mu.size()).cuda()\n",
    "        std = torch.exp(logvar * 0.5)\n",
    "        z = epsilon * std + mu\n",
    "        return z\n",
    "\n",
    "    def decode(self, input):\n",
    "        # implementation goes here\n",
    "        out = self.decoder_fc(input)\n",
    "        out = out.reshape(-1, self.n_channels * 8, 4, 4)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        y must be one-hot\n",
    "        \"\"\"\n",
    "        # add n_classes as additional channels\n",
    "        # num_per_batch x n_classes x 1 x 1\n",
    "        channels = y.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        x = torch.cat((x, channels), dim=1)\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample(mu, logvar)\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "\n",
    "        out = self.decode(z)\n",
    "        return mu, logvar, out\n",
    "\n",
    "    def generate(self, n, y):\n",
    "        \"\"\"\n",
    "        y must be one-hot and be of length n\n",
    "        \"\"\"\n",
    "        z = torch.randn(n, self.latent_dim).cuda()\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "def vae_loss(x, out, mu, logvar, beta=1):\n",
    "    # implementation goes here\n",
    "    recons_loss = ((out - x) * (out - x)).sum()\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = recons_loss + beta * kld_loss\n",
    "    return recons_loss, kld_loss, loss\n",
    "\n",
    "class ConditionalConvGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_in_channels, n_classes, img_size):\n",
    "        \"\"\"\n",
    "        assume img has same height and width\n",
    "        \"\"\"\n",
    "        super(ConditionalConvGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        n_channels = 16 # tuneable hyperparam\n",
    "        self.n_channels = n_channels\n",
    "        self.emb_size = 128\n",
    "        self.flat_dim = n_channels * 8 * 4 * 4\n",
    "\n",
    "        # to embed noise\n",
    "        self.emb = nn.Embedding(n_classes, self.emb_size)\n",
    "        self.decoder_fc = nn.Linear(latent_dim + self.emb_size, self.flat_dim)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 2, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels, n_in_channels, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "    def decode(self, input):\n",
    "        # implementation goes here\n",
    "        out = self.decoder_fc(input)\n",
    "        out = out.reshape(-1, self.n_channels * 8, 4, 4)\n",
    "        out = self.network(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, n, y):\n",
    "        \"\"\"\n",
    "        y must be scalar labels\n",
    "        \"\"\"\n",
    "        z = torch.randn(n, self.latent_dim).cuda()\n",
    "        embed = self.emb(y)\n",
    "        z = torch.cat((z, embed), dim=1)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "class ConditionalConvDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_in_channels, n_classes, img_size):\n",
    "        super(ConditionalConvDiscriminator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        n_channels = 16\n",
    "        self.n_channels = n_channels\n",
    "        self.flat_dim = n_channels * 8 * 4 * 4\n",
    "        self.n_in_channels = n_in_channels\n",
    "\n",
    "        # to embed class labels\n",
    "        self.emb = nn.Embedding(n_classes, img_size * img_size)\n",
    "        self.network = nn.Sequential(\n",
    "            # one more channel from label\n",
    "            nn.Conv2d(n_in_channels + 1, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flat_dim, 1) # scalar output\n",
    "            # no need for sigmoid as we are using BCEWithLogitsLoss\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # implementation goes here\n",
    "        embed = self.emb(y).view(y.shape[0], 1, x.shape[-2], x.shape[-1])\n",
    "        x = torch.cat((x, embed), dim=1)\n",
    "        out = self.network(x)\n",
    "        return out\n",
    "\n",
    "def create_classifier(n_in_channels, n_channels=16):\n",
    "    flat_dim = n_channels * 8 * 4 * 4\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(n_in_channels, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_dim, 1),\n",
    "            nn.Sigmoid() # scalar output, use BCELoss\n",
    "        )\n",
    "    return model.cuda()\n",
    "\n",
    "def train_classifier(epoch, model, opt, criterion, train_loader, dev_loader, writer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        data = data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        preds = model(data)\n",
    "        loss = criterion(preds.squeeze(), labels.float())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # end of epoch, eval on dev and record stats\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # a single batch\n",
    "        for data, labels in dev_loader:\n",
    "            x = data.cuda()\n",
    "            y = labels.cuda()\n",
    "            preds = model(x).squeeze()\n",
    "            loss = criterion(preds, y.float())\n",
    "            dev_auc = roc_auc_score(labels, preds.cpu())\n",
    "    writer.add_scalars('loss',\n",
    "                       {'train': epoch_loss, 'dev': loss.item()},\n",
    "                       epoch)\n",
    "    writer.add_scalar('AUC/dev', dev_auc, epoch)\n",
    "    # save model\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'opt_state_dict': opt.state_dict()\n",
    "            },\n",
    "        os.path.join(OUTDIR, EXPERIMENT, 'model_{}.pth'.format(epoch)))\n",
    "    return dev_auc # save model with best dev auc\n",
    "\n",
    "def train_cvae(epoch, model, opt, loader, writer):\n",
    "    model.train()\n",
    "    epoch_recons_loss = 0\n",
    "    epoch_kld_loss = 0\n",
    "    epoch_loss = 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.cuda()\n",
    "        # y: one-hot labels\n",
    "        y = F.one_hot(y, N_CLASSES).cuda()\n",
    "\n",
    "        mu, logvar, out = model(x, y)\n",
    "        recons_loss, kld_loss, loss = vae_loss(x, out, mu, logvar, beta)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if i == 0: # first batch, record and generate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_recons_loss += recons_loss.item()\n",
    "                epoch_kld_loss += kld_loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                data = vae.generate(LABELS_ONEHOT.shape[0], LABELS_ONEHOT)\n",
    "            grid_img = torchvision.utils.make_grid(data, nrow=N_ROW_IMG, normalize=True)\n",
    "            writer.add_image('generated image', grid_img, epoch)\n",
    "\n",
    "    writer.add_scalar('reconstruction loss', epoch_recons_loss, epoch)\n",
    "    writer.add_scalar('KL-Divergence loss', epoch_kld_loss, epoch)\n",
    "    writer.add_scalar('total loss', epoch_loss, epoch)\n",
    "    return epoch_kld_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = ConditionalConvVAE(100, N_IN_CHANNELS, N_CLASSES).cuda()\n",
    "opt = torch.optim.Adam(vae.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate\n",
    "same size, num positive, and num negative as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
