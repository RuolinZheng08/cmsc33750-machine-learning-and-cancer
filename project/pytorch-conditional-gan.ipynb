{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "078572cd26bfb8a809b4464ba675fdb3cd418fa7"
   },
   "source": [
    "## PyTorch Conditional GAN\n",
    "\n",
    "This kernel is a PyTorch implementation of [Conditional GAN](https://arxiv.org/abs/1411.1784), which is a GAN that allows you to choose the label of the generated image. The generator and the discriminator are going to be simple feedforward networks, so I guess the images won't be as good as in this [nice kernel](https://www.kaggle.com/sgamez/fashion-ac-gan-with-keras) by [Sergio GÃ¡mez](https://www.kaggle.com/sgamez). I used [this implementation](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py) by [eriklindernoren](https://github.com/eriklindernoren) as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeba-dataset.zip\t\t      t10k-labels-idx1-ubyte\r\n",
      "fashion-mnist_test.csv\t\t      test\r\n",
      "fashion-mnist_train.csv\t\t      train\r\n",
      "fashionmnist.zip\t\t      train-images-idx3-ubyte\r\n",
      "histopathologic-cancer-detection.zip  train_labels.csv\r\n",
      "img_align_celeba\t\t      train-labels-idx1-ubyte\r\n",
      "t10k-images-idx3-ubyte\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75fac5f0577eb3e3e1ee914f7118a0fdd0207951"
   },
   "source": [
    " Let's start by defining a Dataset class:\n",
    "* [Data Loading and Processing Tutorial on PyTorch's documentation](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "* [torchvision](https://github.com/pytorch/vision) has a [built-in class for Fashion MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "e4b8af03f05931d7d0ee171ec9d28701baaab122"
   },
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        fashion_df = pd.read_csv('../../input/fashion-mnist_train.csv')\n",
    "        self.labels = fashion_df.label.values\n",
    "        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, 28, 28)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "c2b45eafe82ac480f57896c8406176d22bbeea89"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACD0lEQVR4nLWSvW+NYRjGr/t+nvfrnJwXPa2PEh8VERHp0omkAwYJYWrCH8BCUonVwNSYhaUTxs6YsaiQSiXSEvTQNNrS056evs/78TzPbRASMfdar+uXXMMP2JQQAGgLAMMyE5d7Rh6/+G90cXK+JTeG78jyZxn7h8TgmxXdMUjHzuyOalFPMPjuT0mC93FX1SlXA8sdtqFvpgwSgAmCWzvW0nrbuGx6idc5C76170MAENjj51qRK6YiFhIXSF40D6Xr7MFgjJiuKshmibWVU9bWo+r7Q/jfh2YjY0QajbxIkkpb2w6U33q8pS2R9L1eh4nTuVfuxNukvTFwsH+1UTQnLwAsuEKVhKX6NPXFmJcrTdtRVNrV8w0QAV+LduilPjdfHl0sda9Pm87Bb5+4BsKxJ4s1F2lJJM/rSVEKKymLuDrC0LguUmgTZosJFV0bcuBCgq6qH1fvEZaXTCSiN6qozla8DU0cOSmpEffzUG9pnSIX1nwnVzqscVK6Toog6+7ikx/KEPCVVwllJi/Kwgd7n98d7JqeSzRxasFss5R7H1DlWbGu7Ja+dK5mpy7zzUcbQ8/GDxiQ4xqLsyYgWOyfHj27oGdHsa91W2URo7IBPODK4ClwGoBmjxZmKMkLzcREJBCbQQHiCSBdQT7GobXeC7yWfOeNB+z/aoLxwwtMjNB7J7ZsngMJyeYoDfwCgE0RBqCiu3sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F32200CF700>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = FashionMNIST()\n",
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "c7a3b05bb018357a2e595d523d80a770e6a20c72"
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "dataset = FashionMNIST(transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccef848d8bd24db1c12e5fec03cdafcf27468a59"
   },
   "source": [
    "Now let's define the generator and the discriminator, which are simple MLPs. I'm going to use an embedding layer for the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "83a994dc0acddf5d2a2ed1b706b88f6e308997dd"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = x.view(x.size(0), 784)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "cf8e8720990e3dbd63bc074b340d15ffc15ddd17"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), 100)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.view(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "0d875fd305440b7572a3100bf4feae72ca57a8a5"
   },
   "outputs": [],
   "source": [
    "generator = Generator().cuda()\n",
    "discriminator = Discriminator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "2f3f7aa2d3d961df8623b93b3770b88f83bc77d0"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "b976b9831f73e1d5da270c4050d380f4fe141933"
   },
   "outputs": [],
   "source": [
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    g_optimizer.zero_grad()\n",
    "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda())\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "#     return g_loss.data[0]\n",
    "    return g_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "ce7aa31b594067c3e3c006944ca16e7cba2c5ed4"
   },
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # train with real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n",
    "    \n",
    "    # train with fake images\n",
    "    z = Variable(torch.randn(batch_size, 100)).cuda()\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda()\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n",
    "    \n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "#     return d_loss.data[0]\n",
    "    return d_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3c434a2dbc65b30dffa090ba0b51be3588763c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "n_critic = 5\n",
    "display_step = 300\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}...'.format(epoch))\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        real_images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        generator.train()\n",
    "        batch_size = real_images.size(0)\n",
    "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
    "                                          generator, d_optimizer, criterion,\n",
    "                                          real_images, labels)\n",
    "        \n",
    "\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "\n",
    "    generator.eval()\n",
    "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
    "    z = Variable(torch.randn(9, 100)).cuda()\n",
    "    labels = Variable(torch.LongTensor(np.arange(9))).cuda()\n",
    "    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d87a0e53fb451a6cab5ad07f06d45eb8387644aa"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a143c8f12edf5fbe6e607185215fad39f8b932b"
   },
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(100, 100)).cuda()\n",
    "labels = Variable(torch.LongTensor([i for _ in range(10) for i in range(10)])).cuda()\n",
    "sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "grid = make_grid(sample_images, nrow=10, normalize=True).permute(1,2,0).numpy()\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.imshow(grid)\n",
    "_ = plt.yticks([])\n",
    "_ = plt.xticks(np.arange(15, 300, 30), ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], rotation=45, fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
