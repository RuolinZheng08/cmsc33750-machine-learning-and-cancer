{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "# %load utils.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print('Cuda:', torch.cuda.is_available())\n",
    "torch.manual_seed(0)\n",
    "#torch.set_deterministic(True)\n",
    "np.random.seed(0)\n",
    "\n",
    "INDIR = '../../input/'\n",
    "OUTDIR = '../../output'\n",
    "N_IN_CHANNELS = 3 # RGB\n",
    "N_CLASSES = 2 # binary classification\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 96\n",
    "CROP_SIZE = 64\n",
    "\n",
    "def imshow(x):\n",
    "    img = x.data.cpu().permute(1, 2, 0).numpy()\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.annotations.id[index] + '.tif'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        y_label = torch.tensor(self.annotations.label[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n",
    "\n",
    "class ConditionalConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, n_in_channels, n_classes):\n",
    "        super(ConditionalConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        n_channels = 16 # tuneable hyperparam\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_in_channels + n_classes, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.flat_dim = n_channels * 8 * 4 * 4\n",
    "\n",
    "        self.mu = nn.Linear(self.flat_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(self.flat_dim, latent_dim)\n",
    "\n",
    "        self.decoder_fc = nn.Linear(latent_dim + n_classes, self.flat_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 2, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels, n_in_channels, 4, 2, 1),\n",
    "        )\n",
    "\n",
    "    def encode(self, input):\n",
    "        # implementation goes here\n",
    "        x = self.encoder(input)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        # implementation goes here\n",
    "        epsilon = torch.normal(0., 1., size=mu.size()).cuda()\n",
    "        std = torch.exp(logvar * 0.5)\n",
    "        z = epsilon * std + mu\n",
    "        return z\n",
    "\n",
    "    def decode(self, input):\n",
    "        # implementation goes here\n",
    "        out = self.decoder_fc(input)\n",
    "        out = out.reshape(-1, self.n_channels * 8, 4, 4)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        y must be one-hot\n",
    "        \"\"\"\n",
    "        # add n_classes as additional channels\n",
    "        # num_per_batch x n_classes x 1 x 1\n",
    "        channels = y.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        x = torch.cat((x, channels), dim=1)\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample(mu, logvar)\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "\n",
    "        out = self.decode(z)\n",
    "        return mu, logvar, out\n",
    "\n",
    "    def generate(self, n, y):\n",
    "        \"\"\"\n",
    "        y must be one-hot and be of length n\n",
    "        \"\"\"\n",
    "        z = torch.randn(n, self.latent_dim).cuda()\n",
    "        z = torch.cat((z, y), dim=1)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "def vae_loss(x, out, mu, logvar, beta=1):\n",
    "    # implementation goes here\n",
    "    recons_loss = ((out - x) * (out - x)).sum()\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    loss = recons_loss + beta * kld_loss\n",
    "    return recons_loss, kld_loss, loss\n",
    "\n",
    "class ConditionalConvGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_in_channels, n_classes, img_size):\n",
    "        \"\"\"\n",
    "        assume img has same height and width\n",
    "        \"\"\"\n",
    "        super(ConditionalConvGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        n_channels = 16 # tuneable hyperparam\n",
    "        self.n_channels = n_channels\n",
    "        self.emb_size = 128\n",
    "        self.flat_dim = n_channels * 8 * 4 * 4\n",
    "\n",
    "        # to embed noise\n",
    "        self.emb = nn.Embedding(n_classes, self.emb_size)\n",
    "        self.decoder_fc = nn.Linear(latent_dim + self.emb_size, self.flat_dim)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.ConvTranspose2d(n_channels * 8, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 4, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels * 2, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_channels, n_in_channels, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "    def decode(self, input):\n",
    "        # implementation goes here\n",
    "        out = self.decoder_fc(input)\n",
    "        out = out.reshape(-1, self.n_channels * 8, 4, 4)\n",
    "        out = self.network(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, n, y):\n",
    "        \"\"\"\n",
    "        y must be scalar labels\n",
    "        \"\"\"\n",
    "        z = torch.randn(n, self.latent_dim).cuda()\n",
    "        embed = self.emb(y)\n",
    "        z = torch.cat((z, embed), dim=1)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "class ConditionalConvDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, n_in_channels, n_classes, img_size):\n",
    "        super(ConditionalConvDiscriminator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        n_channels = 16\n",
    "        self.n_channels = n_channels\n",
    "        self.flat_dim = n_channels * 8 * 4 * 4\n",
    "        self.n_in_channels = n_in_channels\n",
    "\n",
    "        # to embed class labels\n",
    "        self.emb = nn.Embedding(n_classes, img_size * img_size)\n",
    "        self.network = nn.Sequential(\n",
    "            # one more channel from label\n",
    "            nn.Conv2d(n_in_channels + 1, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flat_dim, 1) # scalar output\n",
    "            # no need for sigmoid as we are using BCEWithLogitsLoss\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # implementation goes here\n",
    "        embed = self.emb(y).view(y.shape[0], 1, x.shape[-2], x.shape[-1])\n",
    "        x = torch.cat((x, embed), dim=1)\n",
    "        out = self.network(x)\n",
    "        return out\n",
    "\n",
    "def create_classifier(n_in_channels, n_channels=16):\n",
    "    flat_dim = n_channels * 8 * 4 * 4\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(n_in_channels, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_dim, 1),\n",
    "            nn.Sigmoid() # scalar output, use BCELoss\n",
    "        )\n",
    "    return model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(n_in_channels, n_channels=16):\n",
    "    flat_dim = n_channels * 8 * 4 * 4 \n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(n_in_channels, n_channels, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * 2, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 2, n_channels * 4, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_channels * 4, n_channels * 8, 4, 2, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_dim, 1),\n",
    "            nn.Sigmoid() # scalar output, use BCELoss\n",
    "        )\n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classifier(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '../../input/'\n",
    "N_IN_CHANNELS = 3 # RGB\n",
    "N_CLASSES = 2 # binary classification\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 96\n",
    "CROP_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(CROP_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = TumorDataset(DIR + 'tumor_train_labels.csv',\n",
    "                      DIR + 'tumor_data/', transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = '../../output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(epoch, model, opt, criterion, train_loader, dev_loader, writer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        labels = labels.cuda()\n",
    "        preds = model(data)\n",
    "        loss = criterion(preds.squeeze(), labels.float())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # end of epoch, eval on dev and record stats\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # a single batch\n",
    "        for data, labels in dev_loader:\n",
    "            x = data.cuda()\n",
    "            y = labels.cuda()\n",
    "            preds = model(x).squeeze()\n",
    "            loss = criterion(preds, y.float())\n",
    "            dev_auc = roc_auc_score(labels, preds.cpu())\n",
    "    writer.add_scalars('loss', \n",
    "                       {'train': epoch_loss, 'dev': loss.item()},\n",
    "                       epoch)\n",
    "    writer.add_scalar('AUC/dev', dev_auc, epoch)\n",
    "    # save model\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'opt_state_dict': opt.state_dict()\n",
    "            }, \n",
    "        os.path.join(OUTDIR, EXPERIMENT, 'model_{}.pth'.format(epoch)))\n",
    "    return dev_auc # save model with best dev auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(os.path.join(OUTDIR, EXPERIMENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, _ = random_split(dataset, [10, 10, len(dataset)-20],\n",
    "                          generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_SIZE = len(dataset) // 5\n",
    "DEV_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "dev_loader = DataLoader(dataset=dev, batch_size=DEV_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0\n",
    "for epoch in range(10):\n",
    "    dev_auc = train_classifier(epoch, model, opt, criterion, train_loader, dev_loader, writer)\n",
    "    if dev_auc > best_auc: # save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'opt_state_dict': opt.state_dict()\n",
    "            }, \n",
    "        os.path.join(OUTDIR, EXPERIMENT, 'best_model.pth'))\n",
    "        best_auc = dev_auc \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = torch.load(os.path.join(OUTDIR, EXPERIMENT, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(stuff['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TumorDataset(DIR + 'tumor_test_labels.csv',\n",
    "                      DIR + 'tumor_data/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, _ = random_split(test_dataset, [10, len(test_dataset) - 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (7): Flatten()\n",
       "  (8): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (9): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset),\n",
    "                         shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # a single batch\n",
    "    for data, labels in test_loader:\n",
    "        x = data.cuda()\n",
    "        y = labels.cuda()\n",
    "        preds = model(x).squeeze()\n",
    "        loss = criterion(preds, y.float())\n",
    "        auc = roc_auc_score(labels, preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799999999999999"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfmat = confusion_matrix(labels, np.where(preds.cpu() > 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cfmat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
