{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'tc1.autosave.model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tc1.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Sequential, Model, model_from_json, model_from_yaml\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "EPOCH = 400\n",
    "BATCH = 20\n",
    "CLASSES = 36\n",
    "\n",
    "PL = 60484   # 1 + 60483 these are the width of the RNAseq datasets\n",
    "P     = 60483   # 60483\n",
    "DR    = 0.1      # Dropout rate\n",
    "\n",
    "def load_data():\n",
    "        train_path = 'type_18_300_train.csv'\n",
    "        test_path = 'type_18_300_test.csv'\n",
    "\n",
    "        df_train = (pd.read_csv(train_path,header=None).values).astype('float32')\n",
    "        df_test = (pd.read_csv(test_path,header=None).values).astype('float32')\n",
    "\n",
    "        print('df_train shape:', df_train.shape)\n",
    "        print('df_test shape:', df_test.shape)\n",
    "\n",
    "        df_y_train = df_train[:,0].astype('int')\n",
    "        df_y_test = df_test[:,0].astype('int')\n",
    "\n",
    "        Y_train = np_utils.to_categorical(df_y_train,CLASSES)\n",
    "        Y_test = np_utils.to_categorical(df_y_test,CLASSES)\n",
    "\n",
    "        df_x_train = df_train[:, 1:PL].astype(np.float32)\n",
    "        df_x_test = df_test[:, 1:PL].astype(np.float32)\n",
    "\n",
    "#        X_train = df_x_train.as_matrix()\n",
    "#        X_test = df_x_test.as_matrix()\n",
    "\n",
    "        X_train = df_x_train\n",
    "        X_test = df_x_test\n",
    "\n",
    "        scaler = MaxAbsScaler()\n",
    "        mat = np.concatenate((X_train, X_test), axis=0)\n",
    "        mat = scaler.fit_transform(mat)\n",
    "\n",
    "        X_train = mat[:X_train.shape[0], :]\n",
    "        X_test = mat[X_train.shape[0]:, :]\n",
    "\n",
    "        return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n",
    "\n",
    "x_train_len = X_train.shape[1]\n",
    "\n",
    "# this reshaping is critical for the Conv1D to work\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=20, strides=1, padding='valid', input_shape=(P, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Conv1D(filters=128, kernel_size=10, strides=1, padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# set up a bunch of callbacks to do work during model training..\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='tc1.autosave.model.h5', verbose=0, save_weights_only=False, save_best_only=True)\n",
    "csv_logger = CSVLogger('tc1.training.log')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH,\n",
    "                    epochs=EPOCH,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks = [checkpointer, csv_logger, reduce_lr])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"tc1.model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"tc1.model.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"tc1.model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('tc1.model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model_json = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tc1.model.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a11d09f2b774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load yaml and create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myaml_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tc1.model.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloaded_model_yaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloaded_model_yaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_yaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tc1.model.yaml'"
     ]
    }
   ],
   "source": [
    "# load yaml and create model\n",
    "yaml_file = open('tc1.model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model_yaml = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model_json.load_weights(MODEL)\n",
    "print(\"Loaded json model from disk\")\n",
    "\n",
    "# evaluate json loaded model on test data\n",
    "loaded_model_json.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score_json = loaded_model_json.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('json Test score:', score_json[0])\n",
    "print('json Test accuracy:', score_json[1])\n",
    "\n",
    "print(\"json %s: %.2f%%\" % (loaded_model_json.metrics_names[1], score_json[1]*100))\n",
    "\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model_yaml.load_weights(MODEL)\n",
    "print(\"Loaded yaml model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model_yaml.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score_yaml = loaded_model_yaml.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('yaml Test score:', score_yaml[0])\n",
    "print('yaml Test accuracy:', score_yaml[1])\n",
    "\n",
    "print(\"yaml %s: %.2f%%\" % (loaded_model_yaml.metrics_names[1], score_yaml[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
